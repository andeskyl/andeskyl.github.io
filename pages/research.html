<div class="container mt-3">
    <section class="bg-thistle row p-3">
        <div>
            <h2 class="section-heading text-start">My Research</h2>
            <hr/>
            <h3>Secure Multi-Party Computation</h3>
            <ul>
                <li><div class="paper">
                    <a class="title no-dec" href="https://doi.ieeecomputersociety.org/10.1109/SP63933.2026.00113" target="_blank">Sort, Sweep, Mirror: Batch Private Interval Lookup with Logarithmic Cost</a>
                    <p class="author"><b>Andes Y. L. Kei</b>, Lucien K. L. Ng, Jack P. K. Ma, Sherman S. M. Chow</p>
                    <p class="venue">IEEE S&P 2026 (AR: 12.8% = 118/925 for Cycle 1)</p>
                    <p class="abs-btn"><a href="javascript:void(0);" class="no-dec">View Abstract â–¸</a></p>
                    <p class="abstract hide">
                        Secure multiparty computation often requires table lookups and piecewise polynomial evaluation for efficiency. Hiding both the table and the accessed entry is essential. Existing work on private lookup table (LUT) protocols either incur Î©(mL) computation and communication (Eurocrypt '24), where â„“ is the input bitwidth, m is the output bitwidth, and L = 2^â„“, or are optimized for small tables (NDSS '25). Realizing ð’ª(â„“ log L)$ communication and ð’ª((â„“ + m) log L) computation per query for the first time, we propose a batch private LUT protocol that overcomes these limitations by processing K lookups with ð’ª(K log L) secure comparisons. More generally, our approach extends to private interval LUT (ILUT) enabling efficient interval-based lookups in batches. Applications include private machine learning over a large input range, which small LUTs cannot approximate accurately. Notably, it supports inverse-square-root approximation and secure multiplication common in quantized neural networks, where outputs are dependent on the private model weights. Systematic experiments demonstrate that our protocol reduces communication by 10.56- to 1984-fold over the state of the art for private LUTs with tables ranging from 2^16 to 2^24 entries. Moreover, it supports >2^11 lookups per second for a 2^24-entry table in a local-area network and >2^9 in a wide-area network.
                    </p>
                </div></li>
                <li><div class="paper">
                    <a class="title no-dec" href="https://www.ndss-symposium.org/ndss-paper/shaft-secure-handy-accurate-and-fast-transformer-inference/" target="_blank">SHAFT: Secure, Handy, Accurate, and Fast Transformer Inference</a>
                    <p class="author"><b>Andes Y. L. Kei</b>, Sherman S. M. Chow</p>
                    <p class="venue">NDSS 2025 (AR: 14.5% = 137/936 for Fall Cycle, <b>Distinguished Artifact Award</b>)</p>
                    <p class="abs-btn"><a href="javascript:void(0);" class="no-dec">View Abstract â–¸</a></p>
                    <p class="abstract hide">
                        Adoption of transformer-based machine learning models is growing, raising concerns about sensitive data exposure. Nonetheless, current secure inference solutions incur substantial overhead due to their extensive reliance on non-linear protocols, such as softmax and Gaussian error linear unit (GELU). Driven by numerical stability needs, softmax approximations (e.g., NeurIPS 2021) typically extract the maximum element of an input vector, incurring logarithmic rounds (in the input length). Existing GELU protocols (e.g., S&P 2024) use piecewise approximations with high-degree polynomials that rely heavily on secure multiplications and comparisons, which are expensive. Such complexities also hinder model owners unfamiliar with cryptography from deploying their custom models easily.
                        <br><br>
                        SHAFT, our proposed system, provides a secure, handy, accurate, and fast transformer inference framework for deployment. Highlights of our contributions include 1) the first constant-round softmax protocol for transformers, uniquely combining the benefits of input clipping and characteristics of ordinary differential equations, and 2) a highly accurate GELU protocol on a novel characterization designed for Fourier series approximation. Extending to broader contexts, our new protocols also apply to general neural networks that use softmax as the final layer and to transformer architectures with different activation functions. Remarkably, SHAFT outperforms state-of-the-art SIGMA (PETS 2024), which uses secret sharing, and BumbleBee (NDSS 2025), which additionally uses RLWE-based homomorphic encryption. More specifically, SHAFT minimizes communication by 25-41%. and matches SIGMA's running time while surpassing BumbleBee in running time by 4.6-5.3Ã— on LANs and 2.9-4.4Ã— on WANs. Alongside these improvements, SHAFT attains accuracy comparable to plaintext models, confirming its numerical stability. Next in this progression, SHAFT provides an accessible open-source framework for secure and handy deployment by smoothly integrating with the Hugging Face library (EMNLP Demos 2020).
                    </p>
                </div></li>
            </ul>
            <hr/>
        </div>
        <div>
        <h2>Professional Activities</h2>
            <hr/>
            <ul>
                <li>External Reviewers:
                    <ul>
                        <li>2026: AsiaCCS</li>
                        <li>2025: ARES, AsiaCCS, Asiacrypt, CODASPY, DSC, ESORICS, Indocrypt, NDSS, NSS, PKC, S&P, TDSC</li>
                        <li>2024: ARES, Asiacrypt, CANS, ICICS, ISISC, RAID, TDSC, Usenix Security</li>
                        <li>2023: IJIS</li>
                    </ul>
                </li>
            </ul>
        </div>
    </section>
</div>